---------------------------
UFLDL assignment solution. (Andrew Ng's class)
---------------------------

These are solutions to the exercises up at the Stanford OpenClassroom Deep Learning class and Andrew Ng's UFLDL Tutorial. 

-----------------------------------
1.Sparse Autoencoder:

	starter:   train.m
	implement: sampleIMAGES.m, sparseAutoencoderCost.m, computeNumericalGradient.m
	dataset:   10000 image patches (8*8) sampled from 10 images (512*512)

2.Vectorized implementation:

	starter:   train.m (actually stl_exercise.m)
	implement: already finished in assignment 1 
	dataset:   MNIST images without label (digits 5-9)

3.Preprocessing-PCA and Whitening


4.Softmax Regression:

	starter:   softmaxExercise.m
	implement: softmaxCost.m, computeNumericalGradient
	dataset:   MNIST  * training  digit [0-9], size 60,000*784; （28*28=784）
	                  * testing   digit [0-9], size 10,000*784;
	result:    with-regulaization   without-regulaization

5.Self-Taught Learning and Unsupervised Feature Learning:

	starter:   stl_exercise.m
	implement: 
	dataset:   MNIST * "unlabeledData" for autoencoder pretraining, digit [5-9], size 29,404*784;
                     * "trainData" for softmax pretraining,         digit [0-4], size 15,298*784;
                     * "testData" for testing,                      digit [0-4], size 15,298*784;
	result:    98.254% "replacement representation";
	           98.365% "concatenation representation";
	           98.470% "replacement representation" + train autoencoder with unlabeledData [5-9] and trainData [0-4]


6.Building Deep Networks for Classification:

	starter:   stackedae_exercise.m
	implement: stackedAECost.m, stackedAEPredict.m, stackedAEExercise.m

	dataset:   MNIST * "trainData" for all pretraining,   digit [0-9], size 60,000*784;
                     * "testData" for testing,            digit [0-9], size 10,000*784;
	result:    Before Finetuning Test Accuracy: 87.780%
	           After Finetuning Test Accuracy:  98.390%


7.Learning color features with Sparse Autoencoders:

	starter:   linearDecoderExercise.m
	implement: sparseAutoencoderLinearCost.m.
	dataset:   stlSampledPatches  100,000*8*8 patches (sampled STL-10 dataset)

8.Convolution and Pooling:

	starter:   cnnExercise.m
	implement: cnnConvolve.m, cnnPool.m
	dataset:   reduced STL-10 dataset (4 classes, 64*64*3 images)
                   trainImages size 2000
                   testImages  size 3200
	result:    Accuracy: 80.406%


-----------------------------------

other links:

	https://github.com/greeness/deep-learning
	https://github.com/danluu/UFLDL-tutorial
	https://github.com/dkyang/UFLDL-Tutorial-Exercise

-----------------------------------
Any questions or comments about this code should be sent to email
s2jackson[at]hotmail.com

